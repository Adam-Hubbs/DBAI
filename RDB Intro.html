<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>NA • DBAI</title><script src="deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><link href="deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet"><script src="deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="deps/font-awesome-6.4.2/css/all.min.css" rel="stylesheet"><link href="deps/font-awesome-6.4.2/css/v4-shims.min.css" rel="stylesheet"><script src="deps/headroom-0.11.0/headroom.min.js"></script><script src="deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="deps/search-1.0.0/fuse.min.js"></script><script src="deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="pkgdown.js"></script><meta property="og:title" content="NA"></head><body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-light" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="index.html">DBAI</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.0.0.9030</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto"><li class="nav-item"><a class="nav-link" href="reference/index.html">Reference</a></li>
<li class="nav-item"><a class="nav-link" href="news/index.html">Changelog</a></li>
      </ul><ul class="navbar-nav"><li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="search.json"></form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/Adam-Hubbs/DBAI/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul></div>


  </div>
</nav><div class="container template-title-body">
<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">

      <h1>NA</h1>
      <small class="dont-index">Source: <a href="https://github.com/Adam-Hubbs/DBAI/blob/master/RDB%20Intro.md" class="external-link"><code>RDB Intro.md</code></a></small>
    </div>


<p>DBAI is currently undergoing development. Some of the features may be changed before full stable release.</p>
<div class="section level2">
<h2 id="dbai">DBAI<a class="anchor" aria-label="anchor" href="#dbai"></a></h2>
<p>DBAI is a package for using Large Language Models with R datasets. You can use it to call AI models straight from R without any other knowledge of API’s or Python. This can be useful for sentiment analysis, imputing missing data, creating synthetic data, making predictions and more.</p>
<p>DBAI was created with these two key principles in mind: <em>Transparency</em> and <em>Simplicity</em></p>
<p>Every call to DBAI includes invisible meta-data information that you can access.</p>
<p>DBAI was made for social scientists, not AI/CS researchers. It uses an intuitive interface to return text generation from data. It was made to abstract technical details. For advanced AI research, we recommend you use packages like TensorFlow in Python.</p>
<p>DBAI supports the following model providers: - Open AI - Anthropic - Google</p>
<p>These models can all be accessed with the function <code>llm_generate</code>, or you can access functions specific to each model provider.</p>
<p>All of these models can be called using the same function with the same syntax. The main difference is each of these three companies require their own API Key. An API Key is similar to an ID or Credit Card number for these companies. They use it to authenticate that it is really you making this request, and to track your usage to bill you. API Keys are obtained from the model provider’s website. For more details about obtaining an API Key and the Costs associated, see <em>API Keys</em>.</p>
<ul><li><p><a href="https://platform.openai.com/playground" class="external-link uri">https://platform.openai.com/playground</a></p></li>
<li><p><a href="https://www.anthropic.com/api" class="external-link uri">https://www.anthropic.com/api</a></p></li>
<li><p><a href="https://ai.google.dev/gemini-api" class="external-link uri">https://ai.google.dev/gemini-api</a></p></li>
</ul><div class="section level3">
<h3 id="costs">Costs<a class="anchor" aria-label="anchor" href="#costs"></a></h3>
<p>All of these companies provide reasonable access to their models. Their cost changes frequently, so the most accurate place to go to gauge price is the website for these companies directly. They typically provide cost per 1 Million tokens (roughly equivalent to a syllable or word). For most datasets up to 10,000 rows the cost should be in pennies.</p>
</div>
</div>
<div class="section level2">
<h2 id="rate-limits">Rate Limits<a class="anchor" aria-label="anchor" href="#rate-limits"></a></h2>
<p>Rate limits are restrictions on how many API calls you can make in a given period of time. Usually, this is measured in Requests Per Minute. This can be thought of as how many observations/rows you can use the function on in a minute before it stops working. For the companies we work with, rate limits start at a low limit of 3-15 per minute for the free/cheapest options. Rate limits increase the more money you put on file with that provider. Usually between $50 and $100 will get you rate limits large enough for most datasets. If you get rate limited in the middle of a function call it will save everything that has been done up to that point and tell you when it stopped. You can then rerun the function with <code>repair=TRUE</code> to have it keep going where it left off.</p>
</div>
<div class="section level2">
<h2 id="loading-the-package">Loading the Package<a class="anchor" aria-label="anchor" href="#loading-the-package"></a></h2>
<p>DBAI is hosted on github. You can install packages from github by using the <code>remotes::install_github()</code> function. This is the equivalent to installing a package from CRAN using <code><a href="https://rdrr.io/r/utils/install.packages.html" class="external-link">install.packages()</a></code>, and only needs to be done once.</p>
<p><code>{r} remotes::install_github("Adam-Hubbs/DBAI")</code></p>
<p>To load the package into memory, use the <code><a href="https://rdrr.io/r/base/library.html" class="external-link">library()</a></code> function as you would a package from CRAN.</p>
<p><code>{r} library(DBAI)</code></p>
<p>There is one more step we must do before using this package. We need to set the API Key. We will need to set this information in an environmental variable so the R package can recognize it. This needs to be done once per R session. You only need the API Key for the model provider you want to use. For example, if you only care about access to Claude models then you only need the Anthropic API Key. If you only want to run OpenAI’s models you only need the OpenAI API Key, etc.</p>
<p>```{r} Sys.setenv( OPENAI_API_KEY = ‘XXXXXXXXXXX’ )</p>
<p>Sys.setenv( ANTHROPIC_API_KEY = ‘XXXXXXXXXXXX’ )</p>
<p>Sys.setenv( GOOGLE_API_KEY = ‘XXXXXXXXXXXX’ )</p>
<pre><code>
## Example Data

Let's input some example data and use this function. We'll start with a dataset containing demographic information. We have age, gender, occupation, location, race, and religion information on individuals. For this example, we have taken this information and condensed it into text form in a column called 'demo'. Let's take this information and try to predict who they voted for in the 2020 presidential election.

```{r}
sample_df &lt;- data.frame(
  year = c(1964, 1998, 1979, 1981),
  gender = c("Male", "Female", "Male", "Female"),
  occupation = c("Farmer", "Investment Banker", "Lawyer", "Social Worker"),
  location = c("Kansas", "New York", "Phoenix", "Baltimore"),
  race = c("White", "White", "Hispanic", "Black"),
  religion = c("Evangelical Protestant", "Catholic", "Catholic", "Muslim"),
  demo = c(
    "60 year old white man from Kansas. Is an evangelical protestant and a farmer.",
    "26 year old white female investment banker from New York. Is a Catholic.",
    "45 year old male lawyer from Phoenix. Is a hispanic catholic.",
    "43 year old black female. Works as a social worker in Baltimore and is a practicing muslim."))



prompt &lt;- "I will give you demographic information. I want you to predict who they voted for in the 2020 Presidential election. Make your best guess if you are unsure. Say 'Trump' or 'Biden' only. Do not say anything else."</code></pre>
<p>With our dataset and our prompt, lets call our function.</p>
<p>```{r} return_obj &lt;- llm_generate(source = sample_df, input = “demo”, output = “Vote”, prompt = prompt, model = c(“gpt-3.5-turbo”, “gemini-1.5-flash”), max_tokens = 10)</p>
<p>print(return_obj)</p>
<pre><code>
Here we call out function `gpt()` and tell it the source of our data is "sample_df", column of data we want processed is called "demo". We want it to spit out the results in a column called "Vote", the prompt we are using is "prompt", the model is "gpt-3.5-turbo".

Go ahead and run this and examine the results.

DBAI functions return the text of the completion with meta-data invisibly attached. To view the meta-data attached with a return vector, call `summary()` on the output.

```{r}
result_df &lt;- gpt(source = sample_df, input = "demo", output = "Vote", prompt = prompt, model = "gpt-3.5-turbo")

summary(result_df$Vote)</code></pre>
<p><code>DBAI</code> functions are vectorized and can take multiple prompts at the same time and return multiple results. Let’s extend our first example.</p>
<p>```{r} sample_df &lt;- data.frame( year = c(1964, 1998, 1979, 1981), gender = c(“Male”, “Female”, “Male”, “Female”), occupation = c(“Farmer”, “Investment Banker”, “Lawyer”, “Social Worker”), location = c(“Kansas”, “New York”, “Phoenix”, “Baltimore”), race = c(“White”, “White”, “Hispanic”, “Black”), religion = c(“Evangelical Protestant”, “Catholic”, “Catholic”, “Muslim”), demo = c( “60 year old white man from Kansas. Is an evangelical protestant and a farmer.”, “26 year old white female investment banker from New York. Is a Catholic.”, “45 year old male lawyer from Phoenix. Is a hispanic catholic.”, “43 year old black female. Works as a social worker in Baltimore and is a practicing muslim.”))</p>
<p>prompt &lt;- “I will give you demographic information. I want you to predict who they voted for in the 2020 Presidential election. Make your best guess if you are unsure. Say ‘Trump’ or ‘Biden’ only. Do not say anything else.”</p>
<p>prompt2 &lt;- “I will give you demographic information. I want you to predict what political party they identify with or lean torwards. Make your best guess if you are unsure. Say ‘Republican’ or ‘Democratic’ only. Do not say anything else.”</p>
<p>prompts &lt;- c(prompt, prompt2) outputVec &lt;- c(“Vote”, “Party”)</p>
<pre><code>
Here, we ask it to not only predict the Vote choice, but also predict the Party affiliation of the respondents.

```{r}
sample_df &lt;- gpt(sample_df, input = "demo", output = outputVec, prompt = prompts, model = "gpt-3.5-turbo", return_invisible = TRUE)

print(sample_df)</code></pre>
<p>We only made a few minor changes to the function call. This time we pass a vector of outputs and a vector of prompts. When we run this chunk we will now get two columns of data.</p>
<p>Lets take a look at some of the other argument options. We will discuss four common parameters for Large Language Models, <code>temperature</code>, <code>top_p</code>, <code>top_k</code>, and <code>max_tokens</code>. The first three of these parameters are ways to change how deterministic or random the response will be., and the last deals with how long the response is.</p>
<p><code>Temperature</code> specifies how repetitive and analytical , or creative and random the responses are. For our models, temperature runs between 0 (Analytical) and 2 (Creative). Setting the temperature too low can lead to the exact same results for many calls, regardless of data. It can also be hyper-sensitive to examples in the prompt. Setting the temperature too high can lead the LLM to hallucinate and make up things that aren’t real. By default, the temperature is 1, a happy medium. You may want to play around with temperature to suit your specific needs.</p>
<p><code>Top_p</code> and <code>top_k</code> both have to do with sampling methods that the LLM uses. In order to understand this, let’s take a brief refresher on how Large Language Models work in the first place.</p>
<p>LLM’s work by predicting the next token (roughly analogous to a syllable or word) in a sequence. They are trained on up to virtually all written history. Let’s take the sentence: <code>The man sat next to his</code>. A LLM would then try to predict the next word. There are many possibilities for what could come next, and the LLM knows this, so it produces probabilities for each word that could come next. Let’s say this is what it predicts, along with it’s respective probability. <em>Note, this is purely hypothetical.</em></p>
<table class="table"><thead><tr class="header"><th>Word</th>
<th>Probability</th>
</tr></thead><tbody><tr class="odd"><td>Wife</td>
<td>.35</td>
</tr><tr class="even"><td>Dog</td>
<td>.2</td>
</tr><tr class="odd"><td>Son</td>
<td>.12</td>
</tr><tr class="even"><td>Daughter</td>
<td>.1</td>
</tr><tr class="odd"><td>Friend</td>
<td>.08</td>
</tr><tr class="even"><td>Lawyer</td>
<td>.06</td>
</tr><tr class="odd"><td>Iguana</td>
<td>.05</td>
</tr><tr class="even"><td>Sculpture</td>
<td>.04</td>
</tr></tbody></table><p>Here we see that the most likely next word is ‘Wife’ but there are many other options. <code>Top_k</code> restricts the sample to the top K number of options. For example, if <code>top_k</code> were set to 3 in this example, the LLM would choose between ‘Wife’, ‘Dog’, and ‘Son’. <code>Top_p</code> similarly restricts the sample but instead of giving a fixed number of options, it gives a fixed probability. If out <code>top_p</code> were set to .5, then ‘Wife’ and ‘Dog’ would be considered because they represent the least number of options needed to reach at least a .5 probability.</p>
<p>Lets say the LLM ended up choosing ‘Wife’. Lets look at the wrod after wife for another example of the difference between top_k and top_p. Maybe the next word after ‘Wife’ is a little harder to determine and there were 12 words that combined together reached .5 probability. In this case using the <code>top_p</code> method those 12 words would be options for the LLM. Using the <code>top_k</code> method still only the top 3 would be considered.</p>
<p>It is not advised to use temperature, top_p, or top_k at the same time. Some models will let you and some won’t. Unless you really have to, we recommend only changing temperature.</p>
<p><code>Max_tokens</code> determines how long the maximum response will be. Some models count both the input and output tokens as one, and some only count the output tokens. Setting the max_tokens to a low number like 5 will restrict the response to only a couple words while leaving the max_tokens blank or setting it to a large number like 4,000 will enable the LLM to respond with paragraphs of analysis.</p>
</div>
<div class="section level2">
<h2 id="another-example">Another Example<a class="anchor" aria-label="anchor" href="#another-example"></a></h2>
<p>Let’s look at another example. One where AI tools can really shine - Textual Analysis!</p>
<p>```{r} messages &lt;- data.frame(id = c(1, 2, 1), message = c(“Guns are great”, “i think guns are bad”, “They protect my family and keep the king of england out of my face”))</p>
<p>good &lt;- “I will give you a statement. I want you to tell me if the overall sentiment is ‘Pro-gun’ or ‘Anti-gun’. Say ‘Pro-gun’ or ‘Anti-gun’ only.”</p>
<p>messages &lt;- gpt(messages, input = “message”, output = “GunStance”, prompt = good, model = “gpt-3.5-turbo”, max_tokens = 5, return_invisible = TRUE)</p>
<p>View(messages)</p>
<pre><code>
Here we ask it to categorize statements into either Pro-gun or Anti-gun stances.

## Advanced Features

Let's look at some of the other features we haven't talked about yet.

Sometimes you just want to run the same thing a couple times just to make sure. Perhaps you are asking Claude is there is personally identifiable information in a certain field, or maybe you want to have Gemini deduce a topic from a long comment. For some tasks doing it multiple times is a good indication that your results are internally valid. This is where the `iterations` argument comes in. You can pass iterations any integer value and the function will rerun the call that many times. It will add a \_X at the end of the output column for each iteration \_2 for the second, \_3 for the third, etc. This can stack with having multiple prompts, so thinking back to the Vote and Party example if we also set iterations to equal 2, then it would run both Vote and Party twice.

You may have noticed that there is a handy progress bar that pops up as you run the functions. It shows you helpful information like the number of completed calls out of the total number of calls, and the estimated time remaining. If like to live in suspense however, it is possible to disable this. Just set `progress = FALSE`.

One of the most useful arguments is `repair`. This special repair mode seeks to solve common problems that you will most likely encounter. Perhaps you get rate-limited for trying to run a huge dataset all at once, or maybe OpenAI's servers are busy and they deny your request. For whatever reason you have an error popping up when the function finishes. Do Not Fret! Repair mode is here to save the day! Simply rerun the function with `repair = TRUE`, and the function will pick up right where it left off. Repair mode works regardless of if `return_invisible` was set to True or False (i.e. you can pass it either a dataframe or a llm_completion object) and it will figure it out for you! One caveat thought, the progress bar isn't yet compatible with repair mode, so you won't get to see the pretty bar inch across the screen.

## Documentation

This is a fully built R Package and as such it has documentation available. If you ever want to view the documentation, just call the `?` function. This will give you an overview of the function and detailed information on each argument to the function. When switching model providers be sure to check the defaults and ranges on various model parameters as some differ slightly between model providers.

```{r}
?gpt
?claude
?gemini
?llm_generate
?list_models</code></pre>
</div>
<div class="section level2">
<h2 id="section"></h2>
</div>


  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside></div>


    <footer><div class="pkgdown-footer-left">
  <p>Developed by Adam Hubbs.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.1.</p>
</div>

    </footer></div>





  </body></html>

