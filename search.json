[{"path":"https://adam-hubbs.github.io/DBAI/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2024 DBAI authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://adam-hubbs.github.io/DBAI/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Adam Hubbs. Author, maintainer.","code":""},{"path":"https://adam-hubbs.github.io/DBAI/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Hubbs (2024). DBAI: AI Tools R Datasets. R package version 0.0.0.9012, https://adam-hubbs.github.io/DBAI/, https://github.com/Adam-Hubbs/DBAI.","code":"@Manual{,   title = {DBAI: AI Tools for R Datasets},   author = {Adam Hubbs},   year = {2024},   note = {R package version 0.0.0.9012, https://adam-hubbs.github.io/DBAI/},   url = {https://github.com/Adam-Hubbs/DBAI}, }"},{"path":"https://adam-hubbs.github.io/DBAI/index.html","id":"dbai","dir":"","previous_headings":"","what":"DBAI","title":"AI Tools for R Datasets","text":"DBAI package using AI tools R datasets. can use call AI models straight R without knowledge API’s. can useful sentiment analysis, imputing missing data, creating synthetic data, making predictions . DBAI supports following companies models Open AI via gpt Popular models include gpt-3.5, gpt-4, gpt-4o Access models behind ChatGPT Anthropic via claude versions claude3 Google via gemini Gemini 1.5 family models Gemma models (Deep Infra) Coming Soon Deep Infra via mistral, llama, deep-infra Coming Soon Meta’s Llama models Mistrals open source models Gemma (Google), Wizard (Microsoft), several smaller open source projects addition, models can accessed alias function llm_generate. models can called using similar syntax. main difference four companies require API Key. API Key similar ID Credit Card number companies. use authenticate really making request, track usage bill . API Keys obtained model provider’s website. https://platform.openai.com/playground https://www.anthropic.com/api https://ai.google.dev/gemini-api https://deepinfra.com/","code":""},{"path":"https://adam-hubbs.github.io/DBAI/index.html","id":"costs","dir":"","previous_headings":"DBAI","what":"Costs","title":"AI Tools for R Datasets","text":"companies provide reasonable access models. cost changes frequently, accurate place go gauge price website companies directly. typically provide cost per 1 Million tokens (roughly equivalent syllable word). datasets 10,000 rows cost pennies.","code":""},{"path":"https://adam-hubbs.github.io/DBAI/index.html","id":"rate-limits","dir":"","previous_headings":"","what":"Rate Limits","title":"AI Tools for R Datasets","text":"Rate limits restrictions many API calls can make given period time. Usually, measured Requests Per Minute. can thought many observations/rows can use function minute stops working. companies work , rate limits start low limit 3-15 per minute free/cheapest options. Rate limits increase money put file provider. Usually $50 $100 get rate limits large enough datasets. get rate limited middle function call save everything done point tell stopped. can rerun function repair=TRUE keep going left .","code":""},{"path":"https://adam-hubbs.github.io/DBAI/index.html","id":"loading-the-package","dir":"","previous_headings":"","what":"Loading the Package","title":"AI Tools for R Datasets","text":"DBAI hosted github. can install packages github using remotes::install_github() function. equivalent installing package CRAN using install.packages(), needs done . {r} remotes::install_github(\"Adam-Hubbs/DBAI\") load package memory, use library() function package CRAN. {r} library(DBAI) one step must using package. need set API Key. need set information environmental variable R package can recognize . needs done per R session. need API Key model provider want use. example, care access Claude models need Anthropic API Key. want run OpenAI’s models need OpenAI API Key, etc. ```{r} Sys.setenv( OPENAI_API_KEY = ‘XXXXXXXXXXX’ ) Sys.setenv( ANTHROPIC_API_KEY = ‘XXXXXXXXXXXX’ ) Sys.setenv( GOOGLE_API_KEY = ‘XXXXXXXXXXXX’ ) dataset prompt, lets call function. ```{r} return_obj <- llm_generate(source = sample_df, input = “demo”, output = “Vote”, prompt = prompt, model = c(“gpt-3.5-turbo”, “gemini-1.5-flash”), max_tokens = 10) print(return_obj) DBAI functions vectorized can take multiple prompts time return multiple results. Let’s extend first example. ```{r} sample_df <- data.frame( year = c(1964, 1998, 1979, 1981), gender = c(“Male”, “Female”, “Male”, “Female”), occupation = c(“Farmer”, “Investment Banker”, “Lawyer”, “Social Worker”), location = c(“Kansas”, “New York”, “Phoenix”, “Baltimore”), race = c(“White”, “White”, “Hispanic”, “Black”), religion = c(“Evangelical Protestant”, “Catholic”, “Catholic”, “Muslim”), demo = c( “60 year old white man Kansas. evangelical protestant farmer.”, “26 year old white female investment banker New York. Catholic.”, “45 year old male lawyer Phoenix. hispanic catholic.”, “43 year old black female. Works social worker Baltimore practicing muslim.”)) prompt <- “give demographic information. want predict voted 2020 Presidential election. Make best guess unsure. Say ‘Trump’ ‘Biden’ . say anything else.” prompt2 <- “give demographic information. want predict political party identify lean torwards. Make best guess unsure. Say ‘Republican’ ‘Democratic’ . say anything else.” prompts <- c(prompt, prompt2) outputVec <- c(“Vote”, “Party”) made minor changes function call. time pass vector outputs vector prompts. run chunk now get two columns data. Lets take look argument options. discuss four common parameters Large Language Models, temperature, top_p, top_k, max_tokens. first three parameters ways change deterministic random response ., last deals long response . Temperature specifies repetitive analytical , creative random responses . models, temperature runs 0 (Analytical) 2 (Creative). Setting temperature low can lead exact results many calls, regardless data. can also hyper-sensitive examples prompt. Setting temperature high can lead LLM hallucinate make things aren’t real. default, temperature 1, happy medium. may want play around temperature suit specific needs. Top_p top_k sampling methods LLM uses. order understand , let’s take brief refresher Large Language Models work first place. LLM’s work predicting next token (roughly analogous syllable word) sequence. trained virtually written history. Let’s take sentence: man sat next . LLM try predict next word. many possibilities come next, LLM knows , produces probabilities word come next. Let’s say predicts, along ’s respective probability. Note, purely hypothetical. see likely next word ‘Wife’ many options. Top_k restricts sample top K number options. example, top_k set 3 example, LLM choose ‘Wife’, ‘Dog’, ‘Son’. Top_p similarly restricts sample instead giving fixed number options, gives fixed probability. top_p set .5, ‘Wife’ ‘Dog’ considered represent least number options needed reach least .5 probability. Lets say LLM ended choosing ‘Wife’. Lets look wrod wife another example difference top_k top_p. Maybe next word ‘Wife’ little harder determine 12 words combined together reached .5 probability. case using top_p method 12 words options LLM. Using top_k method still top 3 considered. advised use temperature, top_p, top_k time. models let won’t. Unless really , recommend changing temperature. Max_tokens determines long maximum response . models count input output tokens one, count output tokens. Setting max_tokens low number like 5 restrict response couple words leaving max_tokens blank setting large number like 4,000 enable LLM respond paragraphs analysis.","code":"## Example Data  Let's input some example data and use this function. We'll start with a dataset containing demographic information. We have age, gender, occupation, location, race, and religion information on individuals. For this example, we have taken this information and condensed it into text form in a column called 'demo'. Let's take this information and try to predict who they voted for in the 2020 presidential election.  ```{r} sample_df <- data.frame(   year = c(1964, 1998, 1979, 1981),   gender = c(\"Male\", \"Female\", \"Male\", \"Female\"),   occupation = c(\"Farmer\", \"Investment Banker\", \"Lawyer\", \"Social Worker\"),   location = c(\"Kansas\", \"New York\", \"Phoenix\", \"Baltimore\"),   race = c(\"White\", \"White\", \"Hispanic\", \"Black\"),   religion = c(\"Evangelical Protestant\", \"Catholic\", \"Catholic\", \"Muslim\"),   demo = c(     \"60 year old white man from Kansas. Is an evangelical protestant and a farmer.\",     \"26 year old white female investment banker from New York. Is a Catholic.\",     \"45 year old male lawyer from Phoenix. Is a hispanic catholic.\",     \"43 year old black female. Works as a social worker in Baltimore and is a practicing muslim.\"))            prompt <- \"I will give you demographic information. I want you to predict who they voted for in the 2020 Presidential election. Make your best guess if you are unsure. Say 'Trump' or 'Biden' only. Do not say anything else.\" Here we call out function `gpt()` and tell it the source of our data is \"sample_df\", column of data we want processed is called \"demo\". We want it to spit out the results in a column called \"Vote\", the prompt we are using is \"prompt\", the model is \"gpt-3.5-turbo\".  Go ahead and run this and examine the results.  By default, the DBAI family of models returns an list of class 'llm_completion'. The first item in the list is the returned Data frame with the new column of results. It then returns the prompt, the model, the model provider, the date it was ran on, and a final object containing miscellaneous meta-data that is specific to each provider. llm_completion objects provide a way to track various meta-data.  We can also omit this information and just return the dataframe directly without any additional information. To do this set `return_invisible = TRUE`.  ```{r} result_df <- gpt(source = sample_df, input = \"demo\", output = \"Vote\", prompt = prompt, model = \"gpt-3.5-turbo\", return_invisible = TRUE)  print(result_df) Here, we ask it to not only predict the Vote choice, but also predict the Party affiliation of the respondents.  ```{r} sample_df <- gpt(sample_df, input = \"demo\", output = outputVec, prompt = prompts, model = \"gpt-3.5-turbo\", return_invisible = TRUE)  print(sample_df)"},{"path":"https://adam-hubbs.github.io/DBAI/index.html","id":"another-example","dir":"","previous_headings":"","what":"Another Example","title":"AI Tools for R Datasets","text":"Let’s look another example. One AI tools can really shine - Textual Analysis! ```{r} messages <- data.frame(id = c(1, 2, 1), message = c(“Guns great”, “think guns bad”, “protect family keep king england face”)) good <- “give statement. want tell overall sentiment ‘Pro-gun’ ‘Anti-gun’. Say ‘Pro-gun’ ‘Anti-gun’ .” messages <- gpt(messages, input = “message”, output = “GunStance”, prompt = good, model = “gpt-3.5-turbo”, max_tokens = 5, return_invisible = TRUE) View(messages)","code":"Here we ask it to categorize statements into either Pro-gun or Anti-gun stances.  ## Advanced Features  Let's look at some of the other features we haven't talked about yet.  Sometimes you just want to run the same thing a couple times just to make sure. Perhaps you are asking Claude is there is personally identifiable information in a certain field, or maybe you want to have Gemini deduce a topic from a long comment. For some tasks doing it multiple times is a good indication that your results are internally valid. This is where the `iterations` argument comes in. You can pass iterations any integer value and the function will rerun the call that many times. It will add a \\_X at the end of the output column for each iteration \\_2 for the second, \\_3 for the third, etc. This can stack with having multiple prompts, so thinking back to the Vote and Party example if we also set iterations to equal 2, then it would run both Vote and Party twice.  You may have noticed that there is a handy progress bar that pops up as you run the functions. It shows you helpful information like the number of completed calls out of the total number of calls, and the estimated time remaining. If like to live in suspense however, it is possible to disable this. Just set `progress = FALSE`.  One of the most useful arguments is `repair`. This special repair mode seeks to solve common problems that you will most likely encounter. Perhaps you get rate-limited for trying to run a huge dataset all at once, or maybe OpenAI's servers are busy and they deny your request. For whatever reason you have an error popping up when the function finishes. Do Not Fret! Repair mode is here to save the day! Simply rerun the function with `repair = TRUE`, and the function will pick up right where it left off. Repair mode works regardless of if `return_invisible` was set to True or False (i.e. you can pass it either a dataframe or a llm_completion object) and it will figure it out for you! One caveat thought, the progress bar isn't yet compatible with repair mode, so you won't get to see the pretty bar inch across the screen.  ## Documentation  This is a fully built R Package and as such it has documentation available. If you ever want to view the documentation, just call the `?` function. This will give you an overview of the function and detailed information on each argument to the function. When switching model providers be sure to check the defaults and ranges on various model parameters as some differ slightly between model providers.  ```{r} ?gpt ?claude ?gemini ?llm_generate ?list_models"},{"path":[]},{"path":"https://adam-hubbs.github.io/DBAI/reference/DBAI-package.html","id":null,"dir":"Reference","previous_headings":"","what":"DBAI: AI Tools for R Datasets — DBAI-package","title":"DBAI: AI Tools for R Datasets — DBAI-package","text":"R Wrapper AI API's. package designed make easier access use AI API's R. package designed used OpenAI API, support API's coming soon.","code":""},{"path":[]},{"path":"https://adam-hubbs.github.io/DBAI/reference/DBAI-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"DBAI: AI Tools for R Datasets — DBAI-package","text":"Maintainer: Adam Hubbs amhubbs17@gmail.com","code":""},{"path":"https://adam-hubbs.github.io/DBAI/reference/claude.html","id":null,"dir":"Reference","previous_headings":"","what":"claude — claude","title":"claude — claude","text":"claude","code":""},{"path":"https://adam-hubbs.github.io/DBAI/reference/claude.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"claude — claude","text":"","code":"claude(   source,   input,   output = \"output\",   prompt,   model = \"claude-3-haiku-20240307\",   return_invisible = FALSE,   iterations = 1,   repair = FALSE,   progress = TRUE,   temperature = 1,   top_p = NULL,   top_k = NULL,   anthropic_version = \"2023-06-01\",   max_tokens = 4096,   anthropic_api_key = Sys.getenv(\"ANTHROPIC_API_KEY\") )"},{"path":"https://adam-hubbs.github.io/DBAI/reference/claude.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"claude — claude","text":"source required; source dataframe llm-completion object. input required; column name source dataframe output optional; string column name (vector strings) created source dataframe storing output models. Defaults output. prompt required; string (vector Strings handling multiple operations time) system message sent AI model. model required; length one character vector. return_invisible optional; boolean return just output (TRUE) llm object containing model metadata (FALSE). Defaults FALSE. iterations optional; integer. Number completions generate row. Defaults 1. repair optional; boolean repair NA's output column keep values already present output column output column already created. False overrides data already output column exists. Useful continue computation rate limited. Defaults FALSE. progress optional; length one logical vector. Defaults TRUE. Determines whether show progress bar console. temperature optional; defaults 1; length one numeric vector value 0 (analytical) 1 (creative). top_p optional; length one numeric vector value 0 1. specify temperature top_p, never . recommended, use cases use temperature instead. top_k optional; length one numeric vector integer value greater 0. sample top_k options subsequent token. recommended, use cases use temperature instead. anthropic_version optional; defaults 2023-06-01; length one character vector. Specifies version Anthropic's models. max_tokens optional; defaults (4096 - prompt tokens); length one numeric vector integer value greater 0. anthropic_api_key required; defaults Sys.getenv(\"ANTHROPIC_API_KEY\") (.e., value retrieved .Renviron file); length one character vector. Specifies Anthropic API key.","code":""},{"path":"https://adam-hubbs.github.io/DBAI/reference/claude.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"claude — claude","text":"dataframe output column(s) created","code":""},{"path":"https://adam-hubbs.github.io/DBAI/reference/gemini.html","id":null,"dir":"Reference","previous_headings":"","what":"gemini — gemini","title":"gemini — gemini","text":"gemini","code":""},{"path":"https://adam-hubbs.github.io/DBAI/reference/gemini.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"gemini — gemini","text":"","code":"gemini(   source,   input,   output = \"output\",   prompt,   model = \"gemini-1.5-flash\",   return_invisible = FALSE,   iterations = 1,   repair = FALSE,   progress = TRUE,   temperature = 1,   top_p = NULL,   top_k = NULL,   max_tokens = 4096,   google_api_key = Sys.getenv(\"GOOGLE_API_KEY\") )"},{"path":"https://adam-hubbs.github.io/DBAI/reference/gemini.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"gemini — gemini","text":"source required; source dataframe llm-completion object. input required; column name source dataframe output optional; string column name (vector strings) created source dataframe storing output models. Defaults output. prompt required; string (vector Strings handling multiple operations time) prompts sent AI model. model required; length one character vector. return_invisible optional; boolean return just output (TRUE) llm-completion object containing model metadata (FALSE). Defaults FALSE. iterations optional; integer. Number completions generate prompt. Defaults 1. repair optional; boolean repair NA's output column keep values already present output column output column already created. False overrides data already output column exists. Useful continue computation rate limited. Defaults FALSE. progress optional; length one logical vector. Defaults TRUE. Determines whether show progress bar console. available using repair mode. temperature optional; defaults 1; length one numeric vector value 0 (analytical) 2 (creative). top_p optional; defaults 0.95; length one numeric vector value 0 1. top_k optional; length one numeric vector integer value greater 0. sample top_k options subsequent token. recommended, use cases use temperature instead. value provided, use nucleus sampling. max_tokens optional; length one numeric vector integer value greater 0. Gemini includes output tokens. Defaults 4096. google_api_key required; defaults Sys.getenv(\"GOOGLE_API_KEY\") (.e., value retrieved .Renviron file); length one character vector. Specifies Google API key. Must obtain API Key Google.","code":""},{"path":"https://adam-hubbs.github.io/DBAI/reference/gemini.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"gemini — gemini","text":"dataframe output column(s) created","code":""},{"path":"https://adam-hubbs.github.io/DBAI/reference/gpt.html","id":null,"dir":"Reference","previous_headings":"","what":"gpt — gpt","title":"gpt — gpt","text":"gpt","code":""},{"path":"https://adam-hubbs.github.io/DBAI/reference/gpt.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"gpt — gpt","text":"","code":"gpt(   source,   input,   output = \"output\",   prompt,   model = \"gpt-3.5-turbo\",   return_invisible = FALSE,   iterations = 1,   repair = FALSE,   progress = TRUE,   temperature = 1,   top_p = 1,   n = 1,   presence_penalty = 0,   frequency_penalty = 0,   max_tokens = 4096,   openai_api_key = Sys.getenv(\"OPENAI_API_KEY\"),   openai_organization = NULL )"},{"path":"https://adam-hubbs.github.io/DBAI/reference/gpt.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"gpt — gpt","text":"source required; source dataframe llm-completion object. input required; column name source dataframe output optional; string column name (vector strings) created source dataframe storing output models. Defaults output. prompt required; string (vector Strings handling multiple operations time) prompts sent AI model. model required; length one character vector. return_invisible optional; boolean return just output (TRUE) llm-completion object containing model metadata (FALSE). Defaults FALSE. iterations optional; integer. Number completions generate prompt Defaults 1. repair optional; boolean repair NA's output column keep values already present output column output column already created. False overrides data already output column exists. Useful continue computation rate limited. Defaults FALSE. progress optional; length one logical vector. Defaults TRUE. Determines whether show progress bar console. available using repair mode. temperature optional; defaults 1; length one numeric vector value 0 (analytical) 2 (creative). top_p optional; defaults 1; length one numeric vector value 0 1. n optional; defaults 1; length one numeric vector integer value greater 0. presence_penalty optional; defaults 0; length one numeric vector value -2 2. frequency_penalty optional; defaults 0; length one numeric vector value -2 2. max_tokens optional; defaults (4096 - prompt tokens); length one numeric vector integer value greater 0. openai_api_key required; defaults Sys.getenv(\"OPENAI_API_KEY\") (.e., value retrieved .Renviron file); length one character vector. Specifies OpenAI API key. Must obtain API Key OpenAI. openai_organization optional; defaults NULL; length one character vector. Specifies OpenAI organization.","code":""},{"path":"https://adam-hubbs.github.io/DBAI/reference/gpt.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"gpt — gpt","text":"dataframe output column(s) created","code":""},{"path":"https://adam-hubbs.github.io/DBAI/reference/list_models.html","id":null,"dir":"Reference","previous_headings":"","what":"List models available in the OpenAI API (Anthropic has no API for this. Google does and support for Google will be added soon.) — list_models","title":"List models available in the OpenAI API (Anthropic has no API for this. Google does and support for Google will be added soon.) — list_models","text":"list_models","code":""},{"path":"https://adam-hubbs.github.io/DBAI/reference/list_models.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List models available in the OpenAI API (Anthropic has no API for this. Google does and support for Google will be added soon.) — list_models","text":"","code":"list_models(openai_api_key = Sys.getenv(\"OPENAI_API_KEY\"))"},{"path":"https://adam-hubbs.github.io/DBAI/reference/list_models.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List models available in the OpenAI API (Anthropic has no API for this. Google does and support for Google will be added soon.) — list_models","text":"openai_api_key required; defaults Sys.getenv(\"OPENAI_API_KEY\") (.e., value retrieved .Renviron file); length one character vector. Specifies OpenAI API key.","code":""},{"path":"https://adam-hubbs.github.io/DBAI/reference/list_models.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List models available in the OpenAI API (Anthropic has no API for this. Google does and support for Google will be added soon.) — list_models","text":"list models available OpenAI API.","code":""},{"path":"https://adam-hubbs.github.io/DBAI/reference/llm_generate.html","id":null,"dir":"Reference","previous_headings":"","what":"llm_generate — llm_generate","title":"llm_generate — llm_generate","text":"llm_generate","code":""},{"path":"https://adam-hubbs.github.io/DBAI/reference/llm_generate.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"llm_generate — llm_generate","text":"","code":"llm_generate(   source,   input,   output = \"output\",   prompt,   model = \"gpt-3.5-turbo\",   return_invisible = FALSE,   iterations = 1,   repair = FALSE,   progress = TRUE,   temperature = 1,   top_p = 1,   top_k = NULL,   anthropic_version = \"2023-06-01\",   n = 1,   presence_penalty = 0,   frequency_penalty = 0,   max_tokens = 4096,   openai_api_key = Sys.getenv(\"OPENAI_API_KEY\"),   openai_organization = NULL,   anthropic_api_key = Sys.getenv(\"ANTHROPIC_API_KEY\"),   google_api_key = Sys.getenv(\"GOOGLE_API_KEY\") )"},{"path":"https://adam-hubbs.github.io/DBAI/reference/llm_generate.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"llm_generate — llm_generate","text":"source required; source dataframe llm-completion object. input required; column name source dataframe output optional; string column name (vector strings) created source dataframe storing output models. Defaults output. prompt required; string (vector Strings handling multiple operations time) prompts sent AI model. model required; character vector. return_invisible optional; boolean return just output (TRUE) llm-completion object containing model metadata (FALSE). Defaults FALSE. iterations optional; integer. Number completions generate prompt Defaults 1. repair optional; boolean repair NA's output column keep values already present output column output column already created. False overrides data already output column exists. Useful continue computation rate limited. Defaults FALSE. progress optional; length one logical vector. Defaults TRUE. Determines whether show progress bar console. available using repair mode. temperature optional; defaults 1; length one numeric vector value 0 (analytical) 2 (creative). 0-2 OpenAI Google models, 0-1 Anthropic models. top_p optional; defaults 1; numeric vector value 0 1. top_k optional; numeric vector integer value greater 0. sample top_k options subsequent token. recommended, use cases use temperature instead. anthropic_version optional; defaults 2023-06-01; character vector. Specifies version Anthropic's models. n optional; defaults 1; numeric vector integer value greater 0. presence_penalty optional; defaults 0; numeric vector value -2 2. frequency_penalty optional; defaults 0; numeric vector value -2 2. max_tokens optional; defaults (4096 - prompt tokens); numeric vector integer value greater 0. openai_api_key optional; defaults Sys.getenv(\"OPENAI_API_KEY\") (.e., value retrieved .Renviron file); length one character vector. Specifies OpenAI API key. Must obtain API Key OpenAI. openai_organization optional; defaults NULL; length one character vector. Specifies OpenAI organization. anthropic_api_key optional; defaults Sys.getenv(\"ANTHROPIC_API_KEY\") (.e., value retrieved .Renviron file); length one character vector. Specifies Anthropic API key. google_api_key optional; defaults Sys.getenv(\"GOOGLE_API_KEY\") (.e., value retrieved .Renviron file); length one character vector. Specifies Google API key. Must obtain API Key Google.","code":""},{"path":"https://adam-hubbs.github.io/DBAI/reference/llm_generate.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"llm_generate — llm_generate","text":"dataframe output column(s) created","code":""},{"path":"https://adam-hubbs.github.io/DBAI/reference/print.llm_completion.html","id":null,"dir":"Reference","previous_headings":"","what":"Title: Print Method for llm_completion — print.llm_completion","title":"Title: Print Method for llm_completion — print.llm_completion","text":"Title: Print Method llm_completion","code":""},{"path":"https://adam-hubbs.github.io/DBAI/reference/print.llm_completion.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Title: Print Method for llm_completion — print.llm_completion","text":"","code":"# S3 method for llm_completion print(x)"},{"path":"https://adam-hubbs.github.io/DBAI/reference/print.llm_completion.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Title: Print Method for llm_completion — print.llm_completion","text":"x","code":""},{"path":"https://adam-hubbs.github.io/DBAI/reference/print.llm_completion.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Title: Print Method for llm_completion — print.llm_completion","text":"Invisible","code":""}]
